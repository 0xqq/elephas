{"name":"Elephas","tagline":"Deep learning on Spark with Keras","body":"# Elephas: Keras Deep Learning on Apache Spark\r\n\r\n## Introduction\r\nElephas brings deep learning with [Keras](http://keras.io) to [Apache Spark](http://spark.apache.org). Elephas intends to keep the simplicity and usability of Keras, allowing for fast prototyping of distributed models to run on large data sets.\r\n\r\nἐλέφας is Greek for _ivory_ and an accompanying project to κέρας, meaning _horn_. If this seems weird mentioning, like a bad dream, you should confirm it actually is at the [Keras documentation](https://github.com/fchollet/keras/blob/master/README.md). Elephas also means _elephant_, as in stuffed yellow elephant.\r\n\r\nFor now, elephas is a straight forward parallelization of Keras using Spark's RDDs. Models are initialized on the driver, then serialized and shipped to workers. Spark workers deserialize the model and train their chunk of data before broadcasting their parameters back to the driver. The \"master\" model is updated by averaging worker parameters. \r\n\r\n\r\n## Getting started\r\nCurrently Elephas is not available on PyPI, so you'll have to clone this repository and run\r\n```\r\npython setup.py install\r\n```\r\nfrom within that directory. As this is not the place to explain how to install Spark, you should simply follow the instructions at the [Spark download section](http://spark.apache.org/downloads.html) for a local installation. After installing both Keras and Spark, training a model is done as follows:\r\n\r\n- Create a local pyspark context\r\n```python\r\nfrom pyspark import SparkContext, SparkConf\r\nconf = SparkConf().setAppName('Elephas_App').setMaster('local[8]')\r\nsc = SparkContext(conf=conf)\r\n```\r\n\r\n- Define and compile a Keras model\r\n```python\r\nmodel = Sequential()\r\nmodel.add(Dense(784, 128))\r\nmodel.add(Activation('relu'))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Dense(128, 128))\r\nmodel.add(Activation('relu'))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Dense(128, 10))\r\nmodel.add(Activation('softmax'))\r\n\r\nrms = RMSprop()\r\nmodel.compile(loss='categorical_crossentropy', optimizer=rms)\r\n```\r\n\r\n- Create an RDD from numpy arrays \r\n```python\r\nfrom elephas.utils.rdd_utils import to_simple_rdd\r\nrdd = to_simple_rdd(sc, X_train, Y_train)\r\n```\r\n\r\n- Define a SparkModel from Spark context and Keras model, then simply train it\r\n```python\r\nfrom elephas.spark_model import SparkModel\r\nspark_model = SparkModel(sc,model)\r\nspark_model.train(rdd, nb_epoch=20, batch_size=32, verbose=0, validation_split=0.1)\r\n```\r\n\r\n- Run your script using spark-submit\r\n```\r\nspark-submit --driver-memory 1G ./your_script.py\r\n```\r\nSee the examples folder for a working example.\r\n\r\n## In the pipeline\r\n\r\n- Integration with Spark MLLib:\r\n  - Train and evaluate LabeledPoints RDDs\r\n  - Make elephas models MLLib algorithms\r\n- Integration with Spark ML:\r\n  - Use DataFrames for training\r\n  - Make models ML pipeline components\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}